scenarios = {
    'a': ['Training-a1-a3.csv', 'Testing-a2-a4.csv'],
    'b': ['Training-a1-a2.csv', 'Testing-a1.csv'],
    'c': ['Training-a1-a2.csv', 'Testing-a1-a2-a3.csv']
}

for scenario, (TrainingData, TestingData) in scenarios.items():
    print(f'Please enter the scenario you wish to run - either {scenario}, {scenario.upper()}, or {scenario.capitalize()}:')
    user_input = input()

    if user_input.lower() == scenario.lower():
        break
    else:
        print('Invalid scenario. Please try again.')

# Assign the selected training and testing dataset names
if scenario == 'a':
    TrainingData = scenarios['a'][0]
    TestingData = scenarios['a'][1]
elif scenario == 'b':
    TrainingData = scenarios['b'][0]
    TestingData = scenarios['b'][1]
elif scenario == 'c':
    TrainingData = scenarios['c'][0]
    TestingData = scenarios['c'][1]

X_train, y_train = dp.get_processed_data(TrainingData+'.csv', './categoryMappings/', classType ='binary')
X_test,  y_test  = dp.get_processed_data(TestingData+'.csv',  './categoryMappings/', classType ='binary')
===

print('Please enter the scenario you wish to run: either a, b, or c')
for key in scenarios.keys():
    print(f'  - {key}, or {key.upper()}')

# Loop until user enters a valid choice
while True:
    user_input = input('Enter your choice: ').lower().strip()
    if user_input in scenarios:
        TrainingData, TestingData = scenarios[user_input]
        break
    else:
        print('Invalid scenario. Please try again.')

# Display the selected datasets
print(f"\nYou selected scenario '{user_input.upper()}'.")
print(f"Training data: {TrainingData}")
print(f"Testing data: {TestingData}")

#============================
Test Result:
SA:
Loss [0,1]: 1.3399 Accuracy [0,1]: 0.8192
Print the Confusion Matrix:
[ TN, FP ]
[ FN, TP ]=
[[8716  995]
 [2491 2815]]

SB:
Loss [0,1]: 1.5523 Accuracy [0,1]: 0.9062
Print the Confusion Matrix:
[ TN, FP ]
[ FN, TP ]=
[[8960  751]
 [1175 6285]]

SC:
Loss [0,1]: 1.3092 Accuracy [0,1]: 0.9036
Print the Confusion Matrix:
[ TN, FP ]
[ FN, TP ]=
[[8973  738]
 [2096 7852]]

#==========

Plotting
########################################
# Part 4 - Visualizing Training and Testing Metrics
#######################################

# Import matplot lib libraries for plotting the figures. 
import matplotlib.pyplot as plt

# Keras history keys can be 'accuracy'/'val_accuracy' or 'acc'/'val_acc' 
# depending on Keras version/TensorFlow settings.
history_keys = classifierHistory.history.keys()
print(f"History keys found: {history_keys}")

# Determine the correct key names
if 'accuracy' in history_keys:
    acc_key = 'accuracy'
    val_acc_key = 'val_accuracy'
    loss_key = 'loss'
    val_loss_key = 'val_loss'
elif 'acc' in history_keys:
    acc_key = 'acc'
    val_acc_key = 'val_acc'
    loss_key = 'loss'
    val_loss_key = 'val_loss'
else:
    print("Error: Could not find standard accuracy/loss keys in history. Check Keras version.")
    # Fallback to single line if validation data wasn't recorded properly
    acc_key = 'accuracy'
    loss_key = 'loss'
    val_acc_key = None # Marker that validation data isn't present

# --- Plot the accuracy ---
print('Plot the accuracy (train vs test)')
plt.figure()
plt.plot(classifierHistory.history[acc_key], label='Train Accuracy')

if val_acc_key:
    plt.plot(classifierHistory.history[val_acc_key], label='Test Accuracy') # Added testing data
    plt.legend(loc='lower right')
else:
    plt.legend(['Train Accuracy'], loc='upper left')

plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.savefig('accuracy_train_test.png')
plt.show()


# --- Plot the loss ---
print('Plot the loss (train vs test)')
plt.figure()
plt.plot(classifierHistory.history[loss_key], label='Train Loss')

if val_loss_key:
    plt.plot(classifierHistory.history[val_loss_key], label='Test Loss') # Added testing data
    plt.legend(loc='upper right')
else:
    plt.legend(['Train Loss'], loc='upper left')

plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.savefig('loss_train_test.png')
plt.show()

#====
SA TPR Calculation for unknown attacks
Average TPR (SA unknown attacks): 0.0000

Plotting training-only accuracy and loss...
/home/ubuntu/Downloads/lab-cs-ml-00301/fnn_sample_updated.py:238: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask = subclass_labels == attack_class

#========

print("\n=== Recall (TPR) per Attack Category ===")

for idx, attack_list in enumerate(attacks_subClass):
    category_name = expectedAttackClasses[idx]  # e.g., "DoS (A1)"
    
    # Filter rows that belong to this attack subclass
    mask = df_results['attack_type'].str.lower().isin(attack_list)
    
    if mask.sum() == 0:
        print(f"{category_name}: No samples found in test set.")
        continue
    
    # Extract true/predicted labels for this attack family
    y_true_cat = df_results.loc[mask, 'true_label']
    y_pred_cat = df_results.loc[mask, 'pred_label']
    
    # Compute Recall (TPR)
    recall = recall_score(y_true_cat, y_pred_cat, zero_division=0)
    print(f"{category_name}: Recall (TPR) = {recall:.4f}")

